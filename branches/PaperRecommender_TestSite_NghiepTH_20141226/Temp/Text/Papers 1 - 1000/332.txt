We observe a training set Q composed of l labeled samples {(X1,?1),...,(Xl, ?l )} and u unlabeled samples {X1',...,Xu'}. The labels ?i are independent random variables satisfying Pr{?i=1}=?, Pr{?i=2}=1-?. The labeled observations Xi are independently distributed with conditional density f?i(�) given ?i. Let (X0 ,?0) be a new sample, independently distributed as the samples in the training set. We observe X0 and we wish to infer the classification ?0. In this paper we first assume that the distributions f1(�) and f2(�) are given and that the mixing parameter is unknown. We show that the relative value of labeled and unlabeled samples in reducing the risk of optimal classifiers is the ratio of the Fisher informations they carry about the parameter ?. We then assume that two densities g1(�) and g2(�) are given, but we do not know whether g1(�)=f1 (�) and g2(�)=f2(�) or if the opposite holds, nor do we know ?. Thus the learning problem consists of both estimating the optimum partition of the observation space and assigning the classifications to the decision regions. Here, we show that labeled samples are necessary to construct a classification rule and that they are exponentially more valuable than unlabeled samples