11|||Knowledge Discovery from Sparse Pharmacokinetic Data|||In this research effort, we show that the following hypothesis is true: The independently verified sparse information secured from the scientific literature regarding the effects of methyl mercury on mice enables us to predict the effects of the methyl mercury on humans. The Rough Sets methodology is used in this endeavor|||2000
111|||A fusion of rough sets, modified rough sets, and genetic algorithms for hybrid diagnostic systems|||A hybrid classification system is a system composed of several intelligent techniques such that the inherent limitations of one individual technique be compensated for by the strengths of another technique. In this paper, we investigate the outline of a hybrid diagnostic system for Attention Deficit Disorder (ADD) in children. This system uses Rough Sets (RS) and Modified Rough Sets (MRS) to induce rules from examples and then uses our modified genetic algorithms to globalize the rules. Also, the classification capability of this hybrid system was compared with the behavior of (a) another hybrid classification system using RS, MRS, and the “dropping condition” approach, (b) the Interactive Dichotomizer 3 (ID3) approach, and (c) a basic genetic algorithm. The results revealed that the global rules generated by the hybrid system are more effective in classification of the testing dataset than the rules generated by the above approaches.|||1997
112|||Rough Classification|||This article contains a new concept of approximate analysis of data, based on the idea of a “rough” set. The notion of approximate (rough) description of a set is introduced and investigated. The application to medical data analysis is shown as an example.|||1984
113|||Developmental toxicity risk assessment: a rough sets approach|||A rough sets approach was applied to a data set consisting of animal study results and other compound characteristics to generate local and global (certain/possible) sets of rules for prediction of developmental toxicity in human subjects. A modified version of the rough sets approach is proposed to allow the construction of an approximate set of rules to use for prediction in a manner similar to that of discriminant analysis. The modified rough sets approach is superior in predictability to the original form of rough-sets methodology. In comparison to discriminant analysis, modified rough sets (approximate rules) appear to be better in overall classification, sensitivity, positive and negative predictive values. The findings were supported by applying the modified rough sets and discriminant analysis on a test data set generated from the original data set by using a resampling plan.|||1993
114|||A Theory and Methodology of Inductive Learning|||A theory of inductive learning is presented that characterizes it as a heuristic search through a space of symbolic descriptions, generated by an application of certain inference rules to the initial observational statements (the teacher-provided examples of some concepts, or facts about a class of objects or a phenomenon). The inference rules include generalization rules, which perform generalizing transformations on descriptions, and conventional truth-preserving deductive rules (specialization and reformulation rules). The application of the inference rules to descriptions is constrained by problem background knowledge, and guided by criteria evaluating the ‘quality’ of generated inductive assertions.Based on this theory, a general methodology for learning structural descriptions from examples, called star, is described and illustrated by a problem from the area of conceptual data analysis|||1983
115|||Rough classification|||This article contains a new concept of approximate analysis of data, based on the idea of a “rough” set. The notion of approximate (rough) description of a set is introduced and investigated. The application to medical data analysis is shown as an example.|||1999
116|||LERS-a system for learning from examples based on rough sets|||The paper presents the system LERS for rule induction. The system handles inconsistencies in the input data due to its usage of rough set theory principle. Rough set theory is especially well suited to deal with inconsistencies. In this approach, inconsistencies are not corrected. Instead, system LERS computes lower and upper approximations of each concept. Then it induces certain rules and possible rules. The user has the choice to use the machine learning approach or the knowledge acquisition approach. In the first case, the system induces a single minimal discriminant description for each concept. In the second case, the system induces all rules, each in the minimal form, that can be induced from the input data. In both cases, the user has a choice between the local or global approach.|||1992
12|||The Investigation of Mercury Presence in Human Blood: An Extrapolation from Animal Data Using Neural Networks|||In this research effort a neural network approach was used as a method of extrapolating the presence of mercury in human blood from animal data. We also investigated the effect of different data representations (As-is, Category, Simple binary, Thermometer, and Flag) on the model performance. In addition, we used the Rough Sets methodology to identify the redundant independent variables and then examined the proposed extrapolation model performance for a reduced set of independent variables. Moreover, a quality measure was introduced that revealed that the proposed extrapolation model performed extremely well for the Thermometer data representation.|||2002
121|||Pattern development for vessel accidents: a comparison of statistical and neural computing techniques|||This paper describes a sample of over 900 vessel accidents that occurred on the lower Mississippi River. Two different techniques, one statistical and the other based on a neural network model, were used to build logical groups of accidents. The objective in building the groups was to maximize between-group variation and minimize within-group variation. The result was groups whose records were as homogenous as possible.A clustering algorithm (i.e., a non-inferential statistical technique) generated sets of three, four and five groups. A Kohenen neural network model (i.e., a self-organizing map) also generated sets of three, four and five groups. The two sets of parallel groups were radically different as to the relative number of records in each group. In other words, when the two sets of groups were constructed by the respective techniques, the membership of each comparable group within the two different sets was substantially different. Not only was the respective record count in each group substantially different, so were the descriptive statistics describing each comparable set of groups.These results have significant implications for marine policy makers. Important policy variables include safety factors such as weather, speed of current, time of operation, and location of accidents, but mandatory utilization of a voluntary vessel tracking service may be subject to debate.|||2001
122|||An Introduction to Computing with Neural Nets|||Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets|||1987
123|||Rough classification with valued closeness relation|||The problem being addressed is that of classification support using decision rules learned from examples. The learning methodology is based on the rough set theory which is particularly well suited to deal with inconsistencies in the set of examples. The rules produced using the rough set theory are categorized into deterministic and non-deterministic depending whether a condition part of the rule is uniquely related with a decision part or not. The classification support is performed by matching a new case to one of decision rules. A possible result is that the new case does not match any of the known rules. Then, a set of rules “nearest” to the description of the new case is presented to the decision maker. In order to find “nearest” rules, a distance measure based on a valued closeness relation, accepting both nominal and ordinal attributes, is used. A medical example illustrates the classification support.|||1994
13|||Discriminant Function Analyses of Liver-Specific Carcinogens|||The ability to predict organ-specific carcinogenicity would aid FDA reviewers in evaluating new chemical applications. A NCTR liver cancer database (NCTRlcdb) containing 999 compounds has been developed with three sets of descriptors. The NCTRlcdb has Cerius2, Molconn-Z, and 13C NMR descriptors for each compound. Each compound in the database was assigned a liver cancer or a nonliver cancer classification. Compounds within the NCTRlcdb were evaluated for liver-specific carcinogenicity using partial least squares principal component discriminant function (PLS-DF) modeling. PLS-DF models based on estimated a priori classification probabilities of 0.29 for liver cancer and 0.71 for noncancer yielded an overall predictability of 70.6% which was comprised of a liver cancer sensitivity of 18.8% and a noncancer specificity of 90.8%. PLS-DF models based on equal a priori classification probabilities, 0.50 for liver cancer and 0.5 for noncancer, yielded an overall predictability of 61.0% which was comprised of a liver cancer sensitivity of 50.5% and a noncancer specificity of 65.3%.|||2004
131|||A fusion of rough sets, modified rough sets, and genetic algorithms for hybrid diagnostic systems|||A hybrid classification system is a system composed of several intelligent techniques such that the inherent limitations of one individual technique be compensated for by the strengths of another technique. In this paper, we investigate the outline of a hybrid diagnostic system for Attention Deficit Disorder (ADD) in children. This system uses Rough Sets (RS) and Modified Rough Sets (MRS) to induce rules from examples and then uses our modified genetic algorithms to globalize the rules. Also, the classification capability of this hybrid system was compared with the behavior of (a) another hybrid classification system using RS, MRS, and the “dropping condition” approach, (b) the Interactive Dichotomizer 3 (ID3) approach, and (c) a basic genetic algorithm. The results revealed that the global rules generated by the hybrid system are more effective in classification of the testing dataset than the rules generated by the above approaches.|||1997
132|||Combining NMR spectral and structural data to form models of polychlorinated dibenzodioxins, dibenzofurans, and biphenyls binding to the AhR|||A three-dimensional quantitative spectrometric data-activity relationship (3D-QSDAR) modeling technique which uses NMR spectral and structural information that is combined in a 3D-connectivity matrix has been developed. A 3D-connectivity matrix was built by displaying all possible assigned carbon NMR chemical shifts, carbon-to-carbon connections, and distances between the carbons. Two-dimensional 13C-13C COSY and 2D slices from the distance dimension of the 3D-connectivity matrix were used to produce a relationship among the 2D spectral patterns for polychlorinated dibenzofurans, dibenzodioxins, and biphenyls (PCDFs, PCDDs, and PCBs respectively) binding to the aryl hydrocarbon receptor (AhR). We refer to this technique as comparative structural connectivity spectral analysis (CoSCoSA) modeling. All CoSCoSA models were developed using forward multiple linear regression analysis of the predicted 13C NMR structure-connectivity spectral bins. A CoSCoSA model for 26 PCDFs had an explained variance (r2) of 0.93 and an average leave-four-out cross-validated variance (q4 2) of 0.89. A CoSCoSA model for 14 PCDDs produced an r2 of 0.90 and an average leave-two-out cross-validated variance (q2 2) of 0.79. One CoSCoSA model for 12 PCBs gave an r2 of 0.91 and an average q2 2 of 0.80. Another CoSCoSA model for all 52 compounds had an r2 of 0.85 and an average q4 2 of 0.52. Major benefits of CoSCoSA modeling include ease of development since the technique does not use molecular docking routines.|||2002
133|||BUILDING AN ORGAN-SPECIFIC CARCINOGENIC DATABASE FOR SAR ANALYSES|||FDA reviewers need a means to rapidly predict organ-specific carcinogenicity to aid in evaluating new chemicals submitted for approval. This research addressed the building of a database to use in developing a predictive model for such an application based on structure–activity relationships (SAR). The Internet availability of the Carcinogenic Potency Database (CPDB) provided a solid foundation on which to base such a model. The addition of molecular structures to the CPDB provided the extra ingredient necessary for SAR analyses. However, the CPDB had to be compressed from a multirecord to a single record per chemical database; multiple records representing each gender, species, route of administration, and organ-specific toxicity had to be summarized into a single record for each study. Multiple studies on a single chemical had to be further reduced based on a hierarchical scheme. Structural cleanup involved removal of all chemicals that would impede the accurate generation of SAR type descriptors from commercial software programs; that is, inorganic chemicals, mixtures, and organometallics were removed. Counterions such as Na, K, sulfates, hydrates, and salts were also removed for structural consistency. Structural modification sometimes resulted in duplicate records that also had to be reduced to a single record based on the hierarchical scheme. The modified database containing 999 chemicals was evaluated for liver-specific carcinogenicity using a variety of analysis techniques. These preliminary analyses all yielded approximately the same results with an overall predictability of about 63%, which was comprised of a sensitivity of about 30% and a specificity of about 77%.|||2004
134|||Hose — a novel substructure code|||A novel system of substructure codes has been developed to characterize the spherical environment of single atoms and complete ring systems. The codes are generated automatically from topologically represented chemical structures and serve to describe structural entities corresponding to spectral parameters uniquely. Their hierarchical order permits desired substructures and the corresponding chemical shifts to be sought in inverted files generated from a larger data base, thereby facilitating the estimation of unknown spectra.|||1978
14|||A Signature-Based Liver Cancer Predictive System|||The predictive system presented in this paper employs both SOM and Hopfield nets to determine whether a given chemical agent causes cancer in the liver. The SOM net performs the clustering of the training set and delivers a signature for each cluster. Hopfield net treats each signature as an exemplar and learns the exemplars. Each record of the test set is considered a corrupted signature. The Hopfield net tries to un-corrupt the test record using learned exemplars and map it to one of the signatures and consequently to the prediction value associated with the signature. Four pairs of training and test sets are used to test the system. To establish the validity of the new predictive system, its performance is compared with the performance of the discriminant analysis and the rough sets methodology applied on the same datasets.|||2005
141|||An introduction to computing with neural nets|||Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets.|||1987
142|||The Prediction of Methylmercury Elimination Half-Life in Humans using Animal Data: A Neural Network/Rough Sets Analysis|||Artificial neural networks and Rough Sets methodology have been utilized to predict human pharmacokinetic elimination half-life data based on animal data training sets. Methylmercury (Hg) pharmacokinetic data was obtained from 37 literature references, which provided data on species, gender, age, weight, route of administration, dose, dose frequency, and elimination half-life based on either whole-body Hg analysis or blood Hg analysis. Data were categorized into various formats for analysis comparisons. Rough Sets methodology was utilized to identify and remove redundant independent variables. Artificial neural networks were used to produce models based on the animal data," which were in turn used to predict and compare to the human elimination half-life values. These neural network predictions were compared to allometric graphical plots of the same data. The best artificial neural network prediction was based on a ""thermometer"" categorical representation of the data.|||2003
21|||The Implementation of a Robotic Replanning Framework|||In this study, the implementation of a previously proposed robotic replanning framework is presented. The proposed framework integrates a high level replanning paradigm into a three layer robotic architecture. There has been a great deal of studies on managing unexpected events at lower two levels of three layer architectures but doing replanning at highest level still needs investigation. Supporting replanning with real-time vision feedback from working environment and integrating a learning mechanism as a basis increases the success ratio.|||2002
211|||Integrating Planning and Learning: The PRODIGY Architecture 1|||Planning is a complex reasoning task that is well suited for the study of improving performance and knowledge by learning, i.e. by accumulation and interpretation of planning experience. PRODIGY is an architecture that integrates planning with multiple learning mechanisms. Learning occurs at the planner’s decision points and integration in PRODIGY is achieved via mutually interpretable knowledge structures. This article describes the PRODIGY planner, briefly reports on several learning modules developed earlier along the project, and presents in more detail two recently explored methods to learn to generate plans of better quality. We introduce the techniques, illustrate them with comprehensive examples, and show preliminary empirical results. The article also includes a retrospective discussion of the characteristics of the overall PRODIGY architecture and discusses their evolution within the goal of the project of building a large and robust integrated planning and learning system.|||1995
212|||2Planning for Contingencies: A Decision-based Approach|||A fundamental assumption made by classical AI planners is that there is no uncertainty in the world: the planner has full knowledge of the conditions under which the plan will be executed and the outcome of every action is fully predictable. These planners cannot therefore construct contingency plans, i.e., plans in which different actions are performed in different circumstances. In this paper we discuss some issues that arise in the representation and construction of contingency plans and describe Cassandra, a partial-order contingency planner. Cassandra uses explicit decision-steps that enable the agent executing the plan to decide which plan branch to follow. The decision-steps in a plan result in subgoals to acquire knowledge, which are planned for in the same way as any other subgoals. Cassandra thus distinguishes the process of gathering information from the process of making decisions. The explicit representation of decisions in Cassandra allows a coherent approach to the problems of contingent planning, and provides a solid base for extensions such as the use of different decision-making procedures.|||1996
213|||Xavier: a robot navigation architecture based on partially observable markov decision process models||| Autonomous mobile robots need very reliable navigation capabilities in order to operate unat-tended for long periods of time. We present a technique for achieving this goal that uses partially observable Markov decision process models (POMDPs) to explicitly model navigation uncertainty, including actuator and sensor uncertainty and approximate knowledge of the environment. This allows the robot to maintain a probability distribution over its current pose. Thus, while the robot rarely knows exactly where it is, it always has some belief as to what its true pose is, and is never completely lost. We present a navigation architecture based on POMDPs that provides a uniform framework with an established theoretical foundation for pose estimation, path planning, robot control during navigation, and learning. Our experiments show that this architecture indeed leads to robust corridor navigation for an actual indoor mobile robot.|||1998
214|||A NEW METHODOLOGY FOR DEALING WITH UNCERTAINTY IN ROBOTIC TASKS|||A new method that evaluates the execution of obotic opeations at the task level is poposed. Taditional obot contolles use vaiety of feedback loops at motion level to solve dynamical contol and motion level planning poblems asssuming that the oiginal task plan is neve modified. Howeve, a dastical change in the opeation envionment of the obot might as well affect the sequence of tasks to be executed. In such a case, modifying motion plan may not be sufficient to adapt to the unexpected change in the envionment. In this study, a planning envionment that allows the oiginal task plan to be modified o entiely changed accoding to the changing envionmental conditions is poposed. A heuistic appoach is taken to decide on whethe the oiginal plan should be modified o an entiely new plan should be geneated. A simple case on pick-and-place sequencing of blocks-wold is studied to demonstate the idea.|||1999
22|||A Leaning Based Vision Guided Robotic Agent Replanning Famewok and a Case Study|||In this pape, a leaning based obotic eplanning famewok that can handle unexpected events in dynamic wolds is pesented. Existing obotic achitectues use atificial intelligence planning methods, eactive appoaches o hybid appoaches to incease obustness and unexpected event handling capabilities of obotic agents in dynamic envionments. This study pesents a new hybid task planning achitectue that equips obotic agents with highe level of intelligence with an oiginal eplanning method. The eplanning method uses an altenative based action selection mechanism to select the most efficient action path among possible altenative action paths. The method stoes the costs of actions paths fom pevious executions as an expeience and uses it fo impoving its futue action selection decisions. The fact that a continous vision feedback is supplied to the symbolic planning level (highest level of abstaction) is also a contibution of this study since this way, the achitectue detects the pesence of unexpected events, geneates an updated model of the envionment and discads o modifies the existing obsolete action path in case of an unexpected event(s).|||2004
31|||Improving power output for vibration-based energy scavengers|||If pervasive networks of wireless sensor and communication nodes are to achieve their full potential, researchers must develop practical solutions for self-powering these autonomous electronic devices. Batteries, fuel cells, and other fixed energy alternatives are impractical for wireless devices with an expected lifetime of more than 10 years. An alternative is to use devices that generate power by scavenging ambient environment energy. The authors have developed several small vibration-based energy scavengers using piezoelectric materials that can scavenge power from low-level ambient vibration sources. Given appropriate power conditioning and capacitive storage, the resulting power source is sufficient to support networks of ultra-low-power, peer-to-peer wireless nodes. This article describes these devices, as well as device designs that use new, sometimes micro-scale geometries.|||2005
311|||Energy Scavenging For Wireless Sensor Networks With Special Focus On Vibrations|||The vast reduction in size and power consumption of CMOS circuitry has led to a large research effort based around the vision of wireless sensor networks. The proposed networks will be comprised of thousands of small wireless nodes that operate in a multi-hop fashion, replacing long transmission distances with many low power, low cost wireless devices. The result will be the creation of an intelligent environment responding to its inhabitants and ambient conditions. Wireless devices currently being designed and built for use in such environments typically run on batteries. However, as the networks increase in number and the devices decrease in size, the replacement of depleted batteries will not be practical. The cost of replacing batteries in a few devices that make up a small network about once per year is modest. However, the cost of replacing thousands of devices in a single building annually, some of which are in areas difficult to access, is simply not practical. Another approach would be to use a battery that is large enough to last the entire lifetime of the wireless sensor device. However, a battery large enough to last the lifetime of the device would dominate the overall system size and cost, and thus is not very attractive. Alternative methods of powering the devices that will make up the wireless networks are desperately needed.|||2004
312|||Electric-energy generation using variable-capacitive resonator for power-free LSI: efficiency analysis and fundamental experiment|||A power generator based on a vibration-to-electric energy converter using a variable-resonating capacitor is experimentally demonstrated. The generator consists of a complete system with a mechanical-variable capacitor, a charge-transporting LC tank circuit and an externally powered timing-capture controller. A practical design methodology to maximize the efficiency of the vibration-to-electric energy generation system is also described. The efficiency of the generator is estimated based on three factors: the mechanical-energy loss, the charge-transportation loss, and the timing-capture loss. Through the mechanical-energy analysis, the optimum condition for the resonance is found. The parasitic elements in the charge transporter and the timing management of the capture scheme dominate the generation efficiency. These analyses enable the optimum design of the energy-generation system. An experimentally fabricated and measured generator theoretically has a maximum power of 580 nW; the measured power is 120 nW, so conversion efficiency is 21%. This results from a 43% mechanical-energy loss and a 63% charge-transportation loss. The timing-capture scheme is manually determined and externally powered in the experiment, so its efficiency is not considered. With our result, a new system LSI application with an embedded power source can be explored for the ubiquitous computing era.|||2003
313|||A 300-µW 1.9-GHz CMOS oscillator utilizing micromachined resonators|||A low-power low-phase-noise 1.9-GHz RF oscillator is presented. The oscillator employs a single thin-film bulk acoustic wave resonator and was implemented in a standard 0.18-µm CMOS process. This paper addresses design issues involved in codesigning micromachined resonators with CMOS circuitry to realize ultralow-power RF transceiver components. The oscillator achieves a phase-noise performance of -100 dBc/Hz at 10-kHz offset, -120 dBc/Hz at 100-kHz offset, and -140 dBc/Hz at 1-MHz offset. The startup time of the oscillator is less than 1 µs. The oscillator core consumes 300 µA from a 1-V supply.|||2003
314|||Design and fabrication of a new vibration-based electromechanical power generator|||A device is described for generating electrical power from mechanical energy in a vibrating environment. The design utilises an electromagnetic transducer and its operating principle is based on the relative movement of a magnet pole with respect to a coil.The approach is suitable for embedded remote microsystems structures with no physical links to the outside world. Simulation,modelling and test results following fabrication of a first prototype have demonstrated that generation of practical mounts of power within a reasonable space is possible. Power generation of more than 1 mW within a volume of 240 mm3 at a vibration frequency of 320 Hz has been obtained.|||2001
316|||Towards a Piezoelectric Vibration Powered Microgenerator|||As MEMS and Smart Material technologies advance, embedded and remote applications are becoming more widespread. Powering these systems can be a significant engineering problem, as traditional solutions such as batteries are not always appropriate. An inertial generator is developed that uses thick-film piezoelectric technologies to produce electrical power from vibrations in the environment of the device. The device validates the concept, and produces an output of 3 µW. Predictions show that orders of magnitude increase in power output are possible|||2001
317|||Optimized Piezoelectric Energy Harvesting Circuit Using Step-Down Converter in Discontinuous Conduction Mode|||An optimized method of harvesting vibrational energy with a piezoelectric element using a step-down DC-DC converter is presented. In this configuration, the converter regulates the power flow from the piezoelectric element to the desired electronic load. Analysis of the converter in discontinuous current conduction mode results in an expression for the duty cycle-power relationship. Using parameters of the mechanical system, the piezoelectric element," and the converter; the""optimal"" duty cycle can be determined where the harvested power is maximized for the level of mechanical excitation. It is shown that", as the magnitude of the mechanical excitation increases, the optimal duty cycle becomes essentially constant, greatly simplifying the control of the step-down converter. The expression is validated with experimental data showing that the optimal duty cycle can be accurately determined and maximum energy harvesting attained. A circuit is proposed which implements this relationship, and experimental results show that the converter increases the harvested power by approximately 325%.|||2003
318|||Toward self-tuning adaptive vibration-based microgenerators|||he rapidly decreasing size, cost, and power consumption of wireless sensors has opened up the relatively new research field of energy harvesting. Recent years have seen an increasing amount of research on using ambient vibrations as a power source. An important feature of all of these generators is that they depend on the resonance frequency of the generator device being matched with the frequency of the input vibrations. The goal of this paper, therefore," is to explore solutions to the problem of self-tuning vibration based energy harvesters. A distinction is made between ""active"" tuning actuators that must continuously supply power to achieve the resonance frequency change"," and ""passive"" tuning actuators that supply power initially to tune the frequency"," and then are able to ""turn off"" while maintaining the new resonance frequency. This paper analyzes the feasibility of tuning the resonance frequency of vibration based generators with ""active"" tuning actuators. Actuators that can tune the effective stiffness", mass, and damping are analyzed theoretically. Numerical results based for each type of actuator are presented. It is shown that only actuators that tune the effective damping will result in a net increase in power output, and only under the circumstance that no actuation power is needed to add damping. The net increase in power occurs when the mismatch between driving vibrations the mismatch between driving vibrations the resonance frequency of the device is more than 5%. Finally," the theory and numerical results are validated by experiments done on a piezoelectric generator with a smart material ""active"" tuning actuator.|||2004
319|||Can a Coupling Coefficient of a Piezoelectric Device be Higher Than Those of Its Active Material?|||An electromechanical coupling coefficient is a measure of the effectiveness with which a piezoelectric material (or a device employing such a material) converts the energy in an imposed electrical signal to mechanical energy, or vice versa. There are different kinds of material and device coupling coefficients, corresponding to different modes of excitation and response. Device coupling coefficients are properties of the device and, although related to the material coupling coefficients, are generally different from them. It is commonly held that a device coupling coefficient cannot be greater than some corresponding coupling coefficient of the active material used in the device. A class of devices was recently identified in which the apparent coupling coefficient can, in principle, approach 1.0, which corresponds to the limit of perfect electromechanical energy conversion. The key feature of this class of devices is the use of destabilizing mechanical pre-loads to counter inherent stiffness. The approach is illustrated for a symmetric piezoelectric bimorph device: theory predicts a smooth increase of the apparent coupling coefficient with pre-load, approaching 1.0 at the buckling load. An experiment verified the trend of increasing coupling with pre-load: a load corresponding to 50% of the buckling load increased the bimorph coupling coefficient by more than 40%. This approach provides a way to simultaneously increase both the operating displacement and force of a piezoelectric device, distinguishing it from alternatives such as motion amplification. and may allow transducer designers to achieve substantial performance gains for some actuator and sensor devices.|||1997
41|||Controller parametric robustification using observer-based formulation and multimodel design technique|||This note deals with controller robustification using equivalent observer based structure. The purpose of the method is to improve the parametric robustness of an initial controller, synthesized, for example, by using the LQ,H8, or µ technique. The method uses equivalent Luenberger observer formulation and multimodel design procedure for the parametric robustification. First, the initial controller is put under equivalent observer structure. Second, from this observer-based formulation, the controller is reshaped to increase the parametric robustness without paying attention to the closed-loop poles coming from the controller dynamics. Note that during this step, changes in the controller initial performances are minimized. Another advantage of the equivalent observer based formulation lies with the fact that it could directly be used to schedule the controller (dynamic and feedback parts). Finally, the global method (equivalent observer plus robustification) is applied on the robust control of the space shuttle described in µ-analysis and synthesis toolbox.|||2005
411|||Gain scheduling dynamic linear controllers for a nonlinear plant|||For a general type of nonlinear tracking problem, we consider gain scheduling based on a family of linear dynamic controllers designed for a family of plant linearizations about constant operating points. A necessary and sufficient condition for the existence of gain scheduled controllers satisfying appropriate requirements is presented. Using this condition, the impact of linear controller configuration on the existence of a gain scheduled controller can be assessed, and ad hoc approaches to scheduling can be analyzed|||1995
412|||Compensator synthesis using (C,A,"B)-pairs|||This paper gives a general framework (in the geometric style) for the design of compensators for linear multivariable systems. Basic is the notion of a ""(C",A,"B)-pair of subspaces."" This concept is newly defined here"," and we give its simplest application (to the problem of disturbance decoupling by observation feedback). We then formulate a general compensator synthesis principle and show how several well-known synthesis techniques can be derived from it. We also show that ""almost all"" compensators can be interpreted as observer-based compensators", and discuss the relation between the compensator problem and the stable cover problem|||1980
413|||Computing the estimator-controller form of a compensator|||A technique is presented for finding the estimator and controller gains to implement a desired compensator in estimator-controller form. The technique is derived by mapping the states of the compensator to those of the estimator-controller via a linear transformation. Using the technique involves finding the real invertible solutions of a non-symmetric steady-state matrix Riccati equation. The Ricatti equation may have zero, two or more real invertible solutions. From each solution a set of estimator and controller gains can be computed. Multiple solutions correspond to the freedom to choose which closed-loop system poles are associated with the controller and which are associated with the estimator. This technique can be applied to multi-input multi-output systems. In an example, the technique is applied to the despin, control system of a dual-spin satellite.|||1985
414|||Exact Observer-Based Structures for Arbitrary Compensators|||n this paper, some new techniques for determining the observer-based or LQG form of any compensator with arbitrary order are discussed. The practical appeal of such techniques is that they allow for a simplied implementation and reduced memory storage of general controllers and offer additional flexibility for handling gain-scheduling and input saturation constraints as compensator states become meaningful variables. The derived observer-based controllers are input-output equivalent to the original controller but with an explicit separated estimation/control structure. Such structures involve both static control and estimation gains with an extra Youla parameter that can be either static or dynamic. The proposed techniques are applicable both in continuous and discrete-time, to full-order controllers, that is, controllers whose order is the same as the plant's order but also to augmented- and reduced-order controllers whose orders are greater or smaller, respectively. Necessary conditi...|||1999
415|||A multimodel-based approach to robust and self-scheduled control design|||This paper presents a design technique that aims at “flattening” the real-µ-curve corresponding to a given initial feedback by combining µ-analysis and multimodel design. µ-analysis is used sequential to detect worst cases. Multimodel design is used for treating simultaneously the set of worst cases. The resulting controller can be nonscheduled or scheduled with respect to measured (slowly) varying parameters. This paper is mostly devoted to multimodel design. The multimodel design problem is written in terms of eigenstructure assignment. Two classes of resolution techniques are presented: first, the dynamic feedback is written in a transfer matrix form, second, it is written as an observer structure|||1998
416|||Gain scheduling dynamic linear controllers for a nonlinear plant|||For a general type of nonlinear tracking problem, we consider gain scheduling based on a family of linear dynamic controllers designed for a family of plant linearizations about constant operating points. A necessary and sufficient condition for the existence of gain scheduled controllers satisfying appropriate requirements is presented. Using this condition, the impact of linear controller configuration on the existence of a gain scheduled controller can be assessed, and ad hoc approaches to scheduling can be analyzed.|||1995
417|||The application of scheduled ${H}_{\infty}$ controllers to a vstol aircraft|||The authors apply H8-designed controllers to a generic VSTOL (vertical and short takeoff and landing) aircraft model GVAM. The design study motivates the use of H 8 techniques, and addresses some of the implementation issues which arise for multivariable and H8-designed controllers. An approach for gain scheduling H8 controllers on the basis of the normalized comprime factor robust stabilization problem formulation used for the H8 design is developed. It utilizes the observer structure unique to this particular robustness optimization. A weighting selection procedure, has been developed for the associated loop-shaping technique used to specify performance. Multivariable controllers pose additional problems in the event of actuator saturations, and a desaturation scheme which accounts for this is applied to the GVAM. A comprehensive control law was developed and evaluated using the Royal Aerospace Establishment piloted simulation facility|||1993
418|||An observer based multimodel control design approach|||The classical eigenstructure assignment technique, generally does not lead to satisfactory robustness. This paper presents a new eigenstructure assignment technique that permits the designer to treat several systems simultaneously, and therefore to meet robustness requirements. Redundancy between measurem ents and observed variables is used to solve eigenstructure assignment equations for several models.|||1999
419|||Tools for flight control robustness assessment Évaluation de la robustesse de lois de pilotage avec les outils de µ-analyse|||The aim of this paper is to emphasize the usefulness of the µ framework for the analysis of the robustness properties of flight control systems. Model uncertainties may correspond, either to parametric uncertainties (e.g. in the aerodynamic model or in the value of the trim point) or to neglected high frequency dynamics (e.g. in the actuators or sensors). We especially show that several classical problems (computation of a phase or delay margin, non-linear analysis of a PIO effect, computation of a frequency domain overshoot) can be extended to an uncertain aircraft model. A flexible model of the aircraft can, moreover, be taken into account.|||1999
42|||Dirac structures and Boundary Control Systems associated with Skew-Symmetric Differential Operators|||Associated with a skew-symmetric linear operator on the spatial domain $[a,b]$ we define a Dirac structure which includes the port variables on the boundary of this spatial domain. This Dirac structure is a subspace of a Hilbert space. Naturally, associated with this Dirac structure is an infinite-dimensional system. We parameterize the boundary port variables for which the \( C_{0} \)-semigroup associated with this system is contractive or unitary. Furthermore, this parameterization is used to split the boundary port variables into inputs and outputs. Similarly, we define a linear port controlled Hamiltonian system associated with the previously defined Dirac structure and a symmetric positive operator defining the energy of the system. We illustrate this theory on the example of the Timoshenko beam|||2005
1|||Continuous Representations of Time-Series Gene Expression Data|||We present algorithms for time-series gene expression analysis that permit the principled estimation of unobserved time-points, clustering, and dataset alignment. Each expression profile is modeled as a cubic spline (piecewise polynomial) that is estimated from the observed data and every time point influences the overall smooth expression curve. We constrain the spline coefficients of genes in the same class to have similar expression patterns, while also allowing for gene specific parameters. We show that unobserved time-points can be reconstructed using our method with 10-15% less error when compared to previous best methods. Our clustering algorithm operates directly on the continuous representations of gene expression profiles, and we demonstrate that this is particularly effective when applied to non- uniformly sampled data. Our continuous alignment algorithm also avoids difficulties encountered by discrete approaches. In particular, our method allows for control of the number of degrees of freedom of the warp through the specification of parameterized functions, which helps to avoid overfitting. We demonstrate that our algorithm produces stable low-error alignments on real expression data and further show a specific application to yeast knockout data that produces biologically meaningful results.|||2003
2|||Evaluation of normalization methods for microarray data|||Background: Microarray technology allows the monitoring of expression levels for thousands of genes simultaneously. This novel technique helps us to understand gene regulation as well as gene by gene interactions more systematically. In the microarray experiment, however, many undesirable systematic variations are observed. Even in replicated experiment, some variations are commonly observed. Normalization is the process of removing some sources of variation which affect the measured gene expression levels. Although a number of normalization methods have been proposed, it has been difficult to decide which methods perform best. Normalization plays an important role in the earlier stage of microarray data analysis. The subsequent analysis results are highly dependent on normalization. Results: In this paper, we use the variability among the replicated slides to compare performance of normalization methods. We also compare normalization methods with regard to bias and mean square error using simulated data. Conclusions: Our results show that intensity-dependent normalization often performs better than global normalization methods, and that linear and nonlinear normalization methods perform similarly. These conclusions are based on analysis of 36 cDNA microarrays of 3,840 genes obtained in an experiment to search for changes in gene expression profiles during neuronal differentiation of cortical stem cells. Simulation studies confirm our findings.|||2003
3|||The FunCat, a functional annotation scheme for systematic classification of proteins from whole genomes|||In this paper, we present the Functional Catalogue (FunCat), a hierarchically structured, organism- independent, flexible and scalable controlled classi- fication system enabling the functional description of proteins from any organism. FunCat has been applied for the manual annotation of prokaryotes, fungi, plants and animals. We describe how FunCat is implemented as a highly efficient and robust tool for the manual and automatic annotation of genomic sequences. Owing to its hierarchical architecture, FunCat has also proved to be useful for many subse- quent downstream bioinformatic applications. This is illustrated by the analysis of large-scale experiments from various investigations in transcriptomics and proteomics, where FunCat was used to project experi- mental data into functional units, as 'gold standard' for functional classification methods, and also served to compare the significance of different experimental methods. Over the last decade, the FunCat has been established as a robust and stable annotation scheme that offers both, meaningful and manageable func- tional classification as well as ease of perception.|||2004
4|||Disruption of Yeast Forkhead-associated Cell Cycle Transcription by Oxidative Stress|||The effects of oxidative stress on yeast cell cycle depend on the stress-exerting agent. We studied the effects of two oxidative stress agents, hydrogen peroxide (HP) and the superoxide-generating agent Menadione (MD). We found that two small co-expressed groups of genes regulated by the Mcm1-Fkh2-Ndd1 transcription regulatory complex are sufficient to account for the difference in the effects of HP and MD on the progress of the cell cycle, namely G1 arrest with MD and an S phase delay followed by a G2/M arrest with HP. Support for this hypothesis is provided by fkh1fkh2 double mutants, which are affected by MD as we find HP affects wild type cells. The apparent involvement of a forkhead protein in HP-induced cell cycle arrest, similar to that reported for C. elegans and human, describes a potentially novel stress-response pathway in yeast.|||2004
5|||Score normalization in multimodal biometric systems|||Multimodal biometric systems consolidate the evidence presented by multiple biometric sources and typically provide better recognition performance compared to systems based on a single biometric modality. Although information fusion in a multimodal system can be performed at various levels, integration at the matching score level is the most common approach due to the ease in accessing and combining the scores generated by different matchers. Since the matching scores output by the various modalities are heterogeneous, score normalization is needed to transform these scores into a common domain, prior to combining them. In this paper, we have studied the performance of different normalization techniques and fusion rules in the context of a multimodal biometric system based on the face, fingerprint and hand-geometry traits of a user. Experiments conducted on a database of 100 users indicate that the application of min-max, z-score, and tanh normalization schemes followed by a simple sum of scores fusion method results in better recognition performance compared to other methods. However, experiments also reveal that the min-max and z- score normalization techniques are sensitive to outliers in the data, highlighting the need for a robust and efficient normalization procedure like the tanh normalization. It was also observed that multimodal systems utilizing user-specific weights perform better compared to systems that assign the same weights to individual biometric traits for all users.|||2005
6|||Major disturbance events in terrestrial ecosystems detected using global satellite data sets|||Abstract. Ecosystem scientists have yet to develop a proven methodology,to monitor and understand major disturbance events and their historical regimes at a global scale. This study was conducted to evaluate patterns in an 18-year record of global satellite observations of vegetation phenology,from the Advanced Very High Resolution Radiometer (AVHRR) as a means to characterize major ecosystem disturbance events and regimes. The fraction absorbed of photosynthetically active radiation (FPAR) by vegetation canopies worldwide,has been computed at a monthly time interval from 1982 to 1999 and gridded at a spatial resolution of 0.5,pixel. We find that nearly 400 Mha of the global land surface could be identified with at least one FPAR-LO event over the 18-year time series. The majority of these potential disturbance events occurred in tropical savanna and shrublands or in boreal forest ecosystem classes. Verification of potential disturbance events from our FPAR-LO analysis was carried out using documented,records of the timing of large-scale wildfires at locations throughout the world. Disturbance regimes were further characterized by association analysis with historical climate anomalies. Assuming,accuracy of the FPAR satellite record to characterize major|||2003
7|||On Kernel-Target Alignment|||Kernel based methods are increasingly being used for data modeling because of their conceptualsimplicity and outstanding performance on many tasks. However, the kernel functionis often chosen using trial-and-error heuristics. In this paper we address the problem ofmeasuring the degree of agreement between a kernel and a learning task. A quantitativemeasure of agreement is important from both a theoretical and practical point of view.|||2002
8|||HOT SAX: Efficiently Finding the Most Unusual Time Series Subsequence|||In this work, we introduce the new problem of finding time series discords. Time series discords are subsequences of a longer time series that are maximally different to all the rest of the time series subsequences. They thus capture the sense of the most unusual subsequence within a time series. Time series discords have many uses for data mining, including improving the quality of clustering, data cleaning, summarization, and anomaly detection. As we will show, discords are particularly attractive as anomaly detectors because they only require one intuitive parameter (the length of the subsequence) unlike most anomaly detection algorithms that typically require many parameters. We evaluate our work with a comprehensive set of experiments. In particular, we demonstrate the utility of discords with objective experiments on domains as diverse as Space Shuttle telemetry monitoring, medicine, surveillance, and industry, and we demonstrate the effectiveness of our discord discovery algorithm with more than one million experiments, on 82 different datasets from diverse domains.|||2005
9|||Characterization of network-wide anomalies in traffic flows|||Detecting and understanding anomalies in IP networks is an open and ill-defined problem. Toward this end, we have recently proposed the subspace method for anomaly diagnosis. In this paper we present the first large-scale exploration of the power of the subspace method when applied to flow traffic. An important aspect of this approach is that it fuses information from flow measurements taken throughout a network. We apply the subspace method to three different types of sampled flow traffic in a large academic network: multivariate timeseries of byte counts, packet counts, and IP-flow counts. We show that each traffic type brings into focus a different set of anomalies via the subspace method. We illustrate and classify the set of anomalies detected. We find that almost all of the anomalies detected represent events of interest to network operators. Furthermore, the anomalies span a remarkably wide spectrum of event types, including denial of service attacks (single-source and distributed), flash crowds, port scanning, downstream traffic engineering, high-rate flows, worm propagation, and network outage|||2004
10|||Efficient Algorithms for Mining Outliers from Large Data Sets|||In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its nearest neighbor. We rank each point on the basis of its distance to its nearest neighbor and declare the top points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.|||2000
40|||Using wireless Ethernet for localization|||IEEE 802.11b wireless Ethernet is rapidly becoming the standard for in-building and short-range wireless communication. Many mobile devices such as mobile robots, laptops and PDAs already use this protocol for wireless communication. Many wireless Ethernet cards measure the signal strength of incoming packets. This paper investigates the feasibility of implementing a localization system using this sensor. Using a Bayesian localization framework, we show experiments demonstrating that off-the-shelf wireless hardware can accurately be used for location sensing and tracking with about one meter precision in a wireless-enabled office building.|||2000
41|||Layered Depth Images|||In this paper we present a set of efficient image based rendering methods capable of rendering multiple frames per second on a PC.The first method warps Sprites with Depth representing smooth surfaces without the gaps found in other techniques. A second method for more general scenes performswarping from an intermediate representation called a Layered Depth Image (LDI). An LDI is a viewof the scene from a single input camera view, but with multiple pixels along each line of sight. The size of the representation grows only linearly with the observed depth complexity in the scene. Moreover,because the LDI data are represented in a single image coordinatesystem, McMillan’s warp ordering algorithm can be successfully adapted. As a result, pixels are drawn in the output image in back-to-front order. No z-buffer is required, so alpha-compositing can be done efficiently without depth sorting. This makes splatting an efficient solution to the resampling problem|||1998
452|||Distinctive Image Features from Scale-Invariant Keypoints|||This paper presents a method for extracting distinctive invariant features from images, which can be used to perform reliable matching between different im- ages of an object or scene. The features are invariant to image scale and ro- tation, and are shown to provide robust matching across a a substantial range of affine distortion, addition of noise, change in 3D viewpoint, and change in illumination. The features are highly distinctive, in the sense that a single fea- ture can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least- squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.|||2004
43|||A Multi-View Approach to Motion and Stereo|||This paper presents a new approach to computing dense depth and motion estimates from multiple images. Rather than computing a single depth or motion map from such a collection, we associate motion or depth estimates with each image in the collection (or at least some subset of the im- ages). This has the advantage that the depth or motion of re- gions occluded in one image will still be represented in some other image. Thus, tasks such as novel view interpolation or motion-compensated prediction can be solved with greater fidelity. Furthermore, the natural variation in appearance between different images can be captured. To formulate mo- tion and structure recovery, we cast the problem as a global optimization over the unknown motion or depth maps, and use robust smoothness constraints to constrain the space of possible solutions. We develop and evaluate some motion and depth estimation algorithms based on this framework.|||1999
44||| Multi-camera Scene Reconstruction via Graph Cuts|||We address the problem of computing the 3-dimensional shape of an arbitrary scene from a set of images taken at known view- points. Multi-camera scene reconstruction is a natural generalization of the stereo matching problem. However, it is much more difficult than stereo, primarily due to the difficulty of reasoning about visibility. In this paper, we take an approach that has yielded excellent results for stereo, namely energy minimization via graph cuts. We first give an en- ergy minimization formulation of the multi-camera scene reconstruction problem. The energy that we minimize treats the input images symmet- rically, handles visibility properly, and imposes spatial smoothness while preserving discontinuities. As the energy function is NP-hard to minimize exactly, we give a graph cut algorithm that computes a local minimum in a strong sense. We handle all camera configurations where voxel col- oring can be used, which is a large and natural class. Experimental data demonstrates the effectiveness of our approach.|||2002
45|||Depth Enhanced Panoramas|||Depth enhanced panoramas are a practical solution to the difficult problem of inside-looking-out modeling. Depth enhanced panoramas extend color panoramas to support viewpoint translation, while retaining their speed, convenience, and low cost. Depth enhanced panoramas are built incrementally from same-center-of-projection dense-color and sparse-depth frames that are acquired, registered, and merged at the rate of 5 frames per second. The evolving depth enhanced panorama is rendered continually to provide immediate operator feedback. The viewpoint translation range is increased by combining multiple depth enhanced panoramas in real time. Depth enhanced panoramas are combined using either a splat-based, disconnected representation, or, at pixel level, using a GPU fragment program. Depth enhanced panoramas are built in minutes with $3,000 hardware and support photorealistic rendering of complex room-sized environments.|||2004
46|||Production programming in the classroom|||Students in programming courses generally write ""toy"" programs that are superficially tested", graded, and then discarded. This approach to teaching programming leaves students unprepared for production programming because the gap between writing toy programs and developing reliable software products is enormous.This paper describes how production programming can be effectively taught to undergraduate students in the classroom. The key to teaching such a course is using Extreme Programming methodology to develop a sustainable open source project with real customers, including the students themselves. Extreme Programming and open source project management are facilitated by a growing collection of free tools such as the JUnit testing framework, the Ant scripting tool, and the SourceForge website for managing open source projects|||2003
47|||Software engineering education in the era of outsourcing, distributed development, and open source software: challenges and opportunities|||As software development becomes increasingly globally distributed, and more software functions are delegated to common open source software (OSS) and commercial off-the-shelf (COTS) components, practicing software engineers face significant challenges for which current software engineering curricula may leave them inadequately prepared. A new multi-faceted distributed development model is emerging that effectively commoditizes many development activities once considered integral to software engineering, while simultaneously requiring practitioners to apply engineering principles in new and often unfamiliar contexts. We discuss the challenges that software engineers face as a direct result of outsourcing and other distributed development approaches that are increasingly being utilized by industry, and some of the key ways we need to evolve software engineering curricula to address these challenges.|||2005
48|||Enriching software engineering courses with service-learning projects and the open-source approach|||Real-world software engineers deal with complex problem. Yet many software engineering courses do not involve projects of enough complexity to give students such experience. We sense that service-learning projects, while difficult to manage and sustain, can serve a crucial role in this regard. Through trials in a senior-level software engineering course, we discovered that the open-source approach works well to enable students to work on large, multiple-term service-learning projects. We developed GROw, a cross-term, cross-team educational software process to meet the challenges of adopting complex, real-world projects in one-term courses, and to sustain service learning.|||2005
49|||Teaching software design with open source software|||When an introductory course on software design and testing was revised, it was decided to use open source software tools as the major examples and objects of study. The goal was to expose students to realistic software systems and give them experience dealing with large quantities of code written by other people. Using open source software also has the beneficial effect of ensuring that students are aware of the open source software movement, and opens up opportunities to discuss topics such as software piracy and ethics.|||2003
50|||Using open source to enhance learning|||Over the course of the last eight years, open source software has emerged as an important stimulus for social and economic change in the communication and information technology (C&IT) industry. Even though the roots of this change stem from education, educators have been slow to adapt. This paper presents an overview of how early adopters have used open source software to enhance learning. In addition, this paper presents the results of the collaboration between the Java society, the Bayamon campus of inter American university of Puerto Rico, and the Puerto Rico government. The result of the collaboration was the creation of SNAP development center, an open source software development project created within inter American university that employs undergraduate student researchers. The objectives of the project have been to increase the number and enhance the quality of graduates with expertise in the C&IT area of interest.|||2005