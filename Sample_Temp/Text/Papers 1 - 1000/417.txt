We present an extensive empirical comparisonof several smoothing techniques inthe domain of language modeling, includingthose described by Jelinek and Mercer(1980), Katz (1987), and Church andGale (1991). We investigate for the rsttime how factors such as training datasize, corpus (e.g., Brown versus Wall StreetJournal), and n-gram order (bigram versustrigram) aect the relative performance ofthese methods, which we measure throughthe cross-entropy of test data. In addition,we...