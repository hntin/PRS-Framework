Information retrieval algorithms leverage various collection statistics to improve performance. Because these statistics are often computed on a relatively small evaluation corpus, we believe using larger, non-evaluation corpora should im- prove performance. Specifically, we advocate incorporating external corpora based on language modeling. We refer to this process as external expansion. When compared to tra- ditional pseudo-relevance feedback techniques, external ex- pansion is more stable across topics and up to 10% more eective in terms of mean average precision. Our results show that using a high quality corpus that is comparable to the evaluation corpus can be as, if not more, eective than using the web. Our results also show that external expansion outperforms simulated relevance feedback. In addition, we propose a method for predicting the extent to which external expansion will improve retrieval performance. Our new mea- sure demonstrates positive correlation with improvements in mean average precision.